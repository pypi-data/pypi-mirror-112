{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `smlb` mini demonstration:<br>Compare different optimization techniques on the same response surface.\n",
    "\n",
    "Scientific Machine Learning Benchmark:<br>\n",
    "A benchmark of regression models in chem- and materials informatics.<br>\n",
    "2019-2020, Citrine Informatics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import smlb\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "Generate a stream of pseudo-random number generators."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prng = smlb.Random(rng=0)\n",
    "seeds = list(np.flip(prng.random.split(30)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The dataset: Friedman-Silverman (1989)\n",
    "\n",
    "Load a 10-dimensional dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from smlb.datasets.synthetic import FriedmanSilverman1989Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = FriedmanSilverman1989Data(dimensions=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Identity learner\n",
    "\n",
    "First, test optimization algorithms against the Friedman-Silverman function itself. An `IdentityLearner` learns to perfectly reproduce the provided dataset (must be of type `VectorSpaceData`). The `ExpectedValue` scorer then returns the value of the function as the score. In this case we attempt to maximize the function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from smlb.learners import IdentityLearner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_identity = IdentityLearner(dataset)\n",
    "score_ev = smlb.ExpectedValue(maximize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizers\n",
    "\n",
    "For this demonstrate, we compare three optimizers: a random sampler, differential evolution as implemented in Scipy, and dual annealing as implemented in Scipy. For the random optimizer we must specify the number of samples. Here we take 1000. To keep things comparable we also specify 1000 function evaluations for dual annealing, though the algorithm will finish out its current iteration when it passes that threshold. Differential evolution does not expose the number of function evaluations as a parameter, but we can set the number of iterations and find that 10 yields good results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from smlb.optimizers import RandomOptimizer, ScipyDifferentialEvolutionOptimizer, ScipyDualAnnealingOptimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_evals = 1e3\n",
    "max_de_iters = 10\n",
    "optimizers = [\n",
    "    RandomOptimizer(num_samples=max_evals, rng=seeds.pop()),\n",
    "    ScipyDifferentialEvolutionOptimizer(rng=seeds.pop(), maxiter=max_de_iters),\n",
    "    ScipyDualAnnealingOptimizer(rng=seeds.pop(), maxfun=max_evals)\n",
    "]\n",
    "labels = [\n",
    "    \"Random Samples\",\n",
    "    \"Differential Evolution\",\n",
    "    \"Dual Annealing\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running the workflow\n",
    "\n",
    "The `OptimizationTrajecotoryPlot` is the only evaluation currently implemented. It draws the median trajectory, shades the quantiles, and optionally draws the extremal results as well. Here we shade the 0.25 to the 0.75 quantile and choose to draw the best/worst trajectory at each point. We run 6 trials for each optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from smlb.workflows import OptimizationTrajectoryComparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_trials = 6\n",
    "fig, ax = plt.subplots()\n",
    "trajectory_plot = smlb.OptimizationTrajectoryPlot(\n",
    "    target=ax,\n",
    "    optimizer_names=labels,\n",
    "    log_scale=True,\n",
    "    quantile_width=0.5,\n",
    "    show_extrama=True\n",
    ")\n",
    "workflow = OptimizationTrajectoryComparison(\n",
    "    data=dataset,\n",
    "    model=model_identity,\n",
    "    scorer=score_ev,\n",
    "    optimizers=optimizers,\n",
    "    evaluations=[trajectory_plot,],\n",
    "    num_trials=num_trials\n",
    ")\n",
    "workflow.run()\n",
    "ax.set_title(\"Friedman-Silverman function (1989)\")\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dual annealing does the best, finding the optimum within a few dozen function evaluations. Differential evolution doesn't do much better than random sampling at first, but pulls ahead after a few hundred evaluations and eventually finds the optimum."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trained Learner\n",
    "\n",
    "Next, we train a learner on some data drawn from the Friedman-Silverman function and optimize a score applied to that model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from smlb.learners import RandomForestRegressionSklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_rf = RandomForestRegressionSklearn(rng=seeds.pop(), uncertainties=\"naive\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_train = 50\n",
    "sampler = smlb.RandomVectorSampler(size=num_train, rng=seeds.pop())\n",
    "training_data = sampler.fit(dataset).apply(dataset)\n",
    "model_rf.fit(training_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example we assume that the goal is to minimize the function. The lowest value in the training data is taken to be the target and we calculate the probability of exceeding that target. The goal of \"minimize\" in the score indicates that the score calculates the probability of being _below_ the target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_value = min(training_data.labels())\n",
    "score_pi = smlb.ProbabilityOfImprovement(target=min_value, goal=\"minimize\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running the workflow\n",
    "\n",
    "Use the same optimizers as above and similar plotting settings. To demonstrate some different settings, here the plot is on a linear scale and does not include the extremal trajectories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_trials = 6\n",
    "fig, ax = plt.subplots()\n",
    "trajectory_plot = smlb.OptimizationTrajectoryPlot(\n",
    "    target=ax,\n",
    "    optimizer_names=labels,\n",
    "    log_scale=False,\n",
    "    quantile_width=0.5,\n",
    "    show_extrama=False\n",
    ")\n",
    "workflow = OptimizationTrajectoryComparison(\n",
    "    data=dataset,\n",
    "    model=model_rf,\n",
    "    scorer=score_pi,\n",
    "    optimizers=optimizers,\n",
    "    evaluations=[trajectory_plot,],\n",
    "    num_trials=num_trials\n",
    ")\n",
    "workflow.run()\n",
    "ax.set_title(\"Friedman-Silverman function (1989)\")\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dual annealing is once again the best performer."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
