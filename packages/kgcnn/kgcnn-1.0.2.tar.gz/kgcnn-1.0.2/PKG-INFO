Metadata-Version: 2.1
Name: kgcnn
Version: 1.0.2
Summary: General Base Layers for Graph Convolutions with tensorflow.keras
Home-page: https://github.com/aimat-lab/gcnn_keras
Author: Patrick Reiser
Author-email: patrick.reiser@kit.edu
License: UNKNOWN
Description: ![GitHub release (latest by date)](https://img.shields.io/github/v/release/aimat-lab/gcnn_keras)
        [![Documentation Status](https://readthedocs.org/projects/kgcnn/badge/?version=latest)](https://kgcnn.readthedocs.io/en/latest/?badge=latest)
        [![PyPI version](https://badge.fury.io/py/kgcnn.svg)](https://badge.fury.io/py/kgcnn)
        ![PyPI - Downloads](https://img.shields.io/pypi/dm/kgcnn)
        [![kgcnn_unit_tests](https://github.com/aimat-lab/gcnn_keras/actions/workflows/unittests.yml/badge.svg)](https://github.com/aimat-lab/gcnn_keras/actions/workflows/unittests.yml)
        
        # Keras Graph Convolutions
        
        A set of layers for graph convolutions in TensorFlow Keras that use RaggedTensors.
        
        # Table of Contents
        * [General](#general)
        * [Requirements](#requirements)
        * [Installation](#installation)
        * [Documentation](#documentation)
        * [Implementation details](#implementation-details)
        * [Literature](#literature)
        * [Datasets](#datasets)
        * [Examples](#examples)
        * [Issues](#issues)
        * [Citing](#citing)
        * [References](#references)
         
        
        <a name="general"></a>
        # General
        
        The package in [kgcnn](kgcnn) contains several layer classes to build up graph convolution models. 
        Some models are given as an example.
        A [documentation](https://kgcnn.readthedocs.io/en/latest/index.html) is generated in [docs](docs).
        This repo is still under construction.
        Any comments, suggestions or help are very welcome!
        
        <a name="requirements"></a>
        # Requirements
        
        For kgcnn, usually the latest version of tensorflow is required, but is listed as extra requirements in the `setup.py` for simplicity. 
        Additional python packages are placed in the `setup.py` requirements and are installed automatically.
        * tensorflow>=2.4.1
        
        <a name="installation"></a>
        # Installation
        
        Clone repository https://github.com/aimat-lab/gcnn_keras and install with editable mode:
        
        ```bash
        pip install -e ./gcnn_keras
        ```
        or latest release via Python Package Index.
        ```bash
        pip install kgcnn
        ```
        <a name="documentation"></a>
        # Documentation
        
        Auto-documentation is generated at https://kgcnn.readthedocs.io/en/latest/index.html .
        
        <a name="implementation-details"></a>
        # Implementation details
        
        ### Representation
        The most frequent usage for graph convolutions is either node or graph classification. As for their size, either a single large graph, e.g. citation network or small (batched) graphs like molecules have to be considered. 
        Graphs can be represented by an index list of connections plus feature information. Typical quantities in tensor format to describe a graph are listed below.
        
        * `nodes`: Node-list of shape `(batch, N, F)` where `N` is the number of nodes and `F` is the node feature dimension.
        * `edges`: Edge-list of shape `(batch, M, F)` where `M` is the number of edges and `F` is the edge feature dimension.
        * `indices`: Connection-list of shape `(batch, M, 2)` where `M` is the number of edges. The indices denote a connection of incoming i and outgoing j node as `(i,j)`.
        * `state`: Graph state information of shape `(batch, F)` where `F` denotes the feature dimension.
         
        A major issue for graphs is their flexible size and shape, when using mini-batches. Here, for a graph implementation in the spirit of keras, the batch dimension should be kept also in between layers. This is realized by using `RaggedTensor`.
        
        
        ### Input
        
        In order to input batched tensors of variable length with keras, either zero-padding plus masking or ragged and sparse tensors can be used. Morover for more flexibility, a dataloader from `tf.keras.utils.Sequence` is often used to input disjoint graph representations. Tools for converting numpy or scipy arrays are found in [utils](kgcnn/utils).
        
        Here, for ragged tensors, the nodelist of shape `(batch, None, F)` and edgelist of shape `(batch, None, Fe)` have one ragged dimension `(None, )`.
        The graph structure is represented by an index-list of shape `(batch, None, 2)` with index of incoming `i` and outgoing `j` node as `(i,j)`. 
        The first index of incoming node `i` is usually expected to be sorted for faster pooling operations, but can also be unsorted (see layer arguments). Furthermore, the graph is directed, so an additional edge with `(j,i)` is required for undirected graphs. A ragged constant can be directly obtained from a list of numpy arrays: `tf.ragged.constant(indices,ragged_rank=1,inner_shape=(2,))` which yields shape `(batch, None, 2)`.
        
        ### Model
        
        Models can be set up in a functional way. Example message passing from fundamental operations:
        
        
        ```python
        import tensorflow.keras as ks
        from kgcnn.layers.gather import GatherNodes
        from kgcnn.layers.keras import Dense, Concatenate  # ragged support
        from kgcnn.layers.pooling import PoolingLocalMessages, PoolingNodes
        
        n = ks.layers.Input(shape=(None, 3), name='node_input', dtype="float32", ragged=True)
        ei = ks.layers.Input(shape=(None, 2), name='edge_index_input', dtype="int64", ragged=True)
        
        n_in_out = GatherNodes()([n, ei])
        node_messages = Dense(10, activation='relu')(n_in_out)
        node_updates = PoolingLocalMessages()([n, node_messages, ei])
        n_node_updates = Concatenate(axis=-1)([n, node_updates])
        n_embedd = Dense(1)(n_node_updates)
        g_embedd = PoolingNodes()(n_embedd)
        
        message_passing = ks.models.Model(inputs=[n, ei], outputs=g_embedd)
        ```
        
        
        
        
        <a name="literature"></a>
        # Literature
        A version of the following models are implemented in [literature](kgcnn/literature):
        * **[GCN](kgcnn/literature/GCN.py)**: [Semi-Supervised Classification with Graph Convolutional Networks](https://arxiv.org/abs/1609.02907) by Kipf et al. (2016)
        * **[INorp](kgcnn/literature/INorp.py)**: [Interaction Networks for Learning about Objects,Relations and Physics](https://arxiv.org/abs/1612.00222) by Battaglia et al. (2016)
        * **[Megnet](kgcnn/literature/Megnet.py)**: [Graph Networks as a Universal Machine Learning Framework for Molecules and Crystals](https://doi.org/10.1021/acs.chemmater.9b01294) by Chen et al. (2019)
        * **[NMPN](kgcnn/literature/NMPN.py)**: [Neural Message Passing for Quantum Chemistry](http://arxiv.org/abs/1704.01212) by Gilmer et al. (2017)
        * **[Schnet](kgcnn/literature/Schnet.py)**: [SchNet – A deep learning architecture for molecules and materials ](https://aip.scitation.org/doi/10.1063/1.5019779) by Schütt et al. (2017)
        * **[Unet](kgcnn/literature/Unet.py)**: [Graph U-Nets](http://proceedings.mlr.press/v97/gao19a/gao19a.pdf) by H. Gao and S. Ji (2019)
        * **[GNNExplainer](kgcnn/literature/GNNExplain.py)**: [GNNExplainer: Generating Explanations for Graph Neural Networks](https://arxiv.org/abs/1903.03894) by Ying et al. (2019)
        * **[GraphSAGE](kgcnn/literature/GraphSAGE.py)**: [Inductive Representation Learning on Large Graphs](http://arxiv.org/abs/1706.02216) by Hamilton et al. (2017)
        * **[GAT](kgcnn/literature/GAT.py)**: [Graph Attention Networks](https://arxiv.org/abs/1710.10903) by Veličković et al. (2018)
        * **[DimeNetPP](kgcnn/literature/DimeNetPP.py)**: [Fast and Uncertainty-Aware Directional Message Passing for Non-Equilibrium Molecules](https://arxiv.org/abs/2011.14115) by Klicpera et al. (2020)
        * **[AttentiveFP](kgcnn/literature/AttentiveFP.py)**: [Pushing the Boundaries of Molecular Representation for Drug Discovery with the Graph Attention Mechanism](https://pubs.acs.org/doi/10.1021/acs.jmedchem.9b00959) by Xiong et al. (2019)
        * **[GIN](kgcnn/literature/GIN.py)**: [How Powerful are Graph Neural Networks?](https://arxiv.org/abs/1810.00826) by Xu et al. (2019)
        * **[PAiNN](kgcnn/literature/PAiNN.py)**: [Equivariant message passing for the prediction of tensorial properties and molecular spectra](https://arxiv.org/pdf/2102.03150.pdf) by Schütt et al. (2020)
        
        <a name="datasets"></a>
        # Datasets
        
        In [data](kgcnn/data) there are simple data handling tools that are used for examples, which includes loading [datasets](kgcnn/data/datasets).
        
        <a name="examples"></a>
        # Examples
        
        A set of example training can be found in [example](examples)
        
        # Issues
        
        Some known issues to be aware of, if using and making new models or layers with `kgcnn`.
        * RaggedTensor can not yet be used as a keras model output (https://github.com/tensorflow/tensorflow/issues/42320), which means only padded tensors can be used for batched node embedding tasks.
        * Using `RaggedTensor`'s for arbitrary ragged rank apart from `kgcnn.layers.keras` can cause significant performance decrease.
        
        <a name="citing"></a>
        # Citing
        
        If you want to cite this repo, refer to our paper:
        
        ```
        @article{REISER2021100095,
        title = {Graph neural networks in TensorFlow-Keras with RaggedTensor representation (kgcnn)},
        journal = {Software Impacts},
        pages = {100095},
        year = {2021},
        issn = {2665-9638},
        doi = {https://doi.org/10.1016/j.simpa.2021.100095},
        url = {https://www.sciencedirect.com/science/article/pii/S266596382100035X},
        author = {Patrick Reiser and Andre Eberhard and Pascal Friederich}
        }
        ```
        
        <a name="references"></a>
        # References
        
        - https://www.tensorflow.org/api_docs/python/tf/RaggedTensor
        
Keywords: materials,science,machine,learning,deep,graph,networks,neural
Platform: UNKNOWN
Classifier: Programming Language :: Python :: 3
Classifier: Operating System :: OS Independent
Classifier: Intended Audience :: Science/Research
Classifier: Topic :: Scientific/Engineering :: Information Analysis
Classifier: Topic :: Scientific/Engineering :: Physics
Classifier: Topic :: Scientific/Engineering :: Chemistry
Classifier: Topic :: Software Development :: Libraries :: Python Modules
Description-Content-Type: text/markdown
Provides-Extra: tf
Provides-Extra: tf_gpu
