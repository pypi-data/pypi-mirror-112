Metadata-Version: 2.1
Name: sk-nlp
Version: 0.1.8
Summary: nlp kit.
Home-page: https://github.com/me/myproject
Author: wengsongxiu
Author-email: wengsongxiu@mastercom.cn
License: MIT
Platform: UNKNOWN
Classifier: License :: OSI Approved :: MIT License
Classifier: Programming Language :: Python
Classifier: Programming Language :: Python :: 3
Classifier: Programming Language :: Python :: 3.6
Classifier: Programming Language :: Python :: Implementation :: CPython
Classifier: Programming Language :: Python :: Implementation :: PyPy
Requires-Python: >=3.6.0
Description-Content-Type: text/markdown
Requires-Dist: numpy
Requires-Dist: scipy
Requires-Dist: tensorflow-gpu
Requires-Dist: bert4keras
Requires-Dist: sk-common


# sk-nlp

[![Travis](https://travis-ci.org/CyberZHG/keras-transformer.svg)](https://travis-ci.org/CyberZHG/keras-transformer)
[![Coverage](https://coveralls.io/repos/github/CyberZHG/keras-transformer/badge.svg?branch=master)](https://coveralls.io/github/CyberZHG/keras-transformer)

![](https://img.shields.io/badge/keras-tensorflow-blue.svg)
![](https://img.shields.io/badge/keras-tf.keras-blue.svg)
![](https://img.shields.io/badge/keras-tf.keras/eager-blue.svg)
![](https://img.shields.io/badge/keras-tf.keras/2.0_beta-blue.svg)



ğŸ“¦ é¡¹ç›®ä»‹ç» (for humans)
=======================

è¿™ä¸ªç¬¬ä¸‰æ–¹ä»“åº“æ˜¯ç”±æ·±åœ³å¸‚åé€šç§‘æŠ€è‚¡ä»½æœ‰é™å…¬å¸AIå›¢é˜Ÿæä¾›çš„ã€‚å›¢é˜Ÿè‡´åŠ›äºä¸ºNLPé¢†åŸŸï¼Œæä¾›ä¸€ä¸ªç¨³å®šå¯é ï¼Œ åŠŸèƒ½å®Œå–„çš„NLPå¸¸è§æ“ä½œã€‚


Installation
-----

```bash
cd your_project
pip install sk-nlp
```

# Content
* sk_nlp package

    * [sk_nlp.nlp_feature_extract package](#sk_nlp.nlp_feature_extract package)

      * [sk_nlp.nlp_feature_extract.feature module](#sk_nlp.nlp_feature_extract.feature module)

      * [sk_nlp.nlp_feature_extract.text_filter module](#sk_nlp.nlp_feature_extract.text_filter module)

      * [sk_nlp.nlp_feature_extract.tokenizer module](#sk_nlp.nlp_feature_extract.tokenizer module)

    * [sk_nlp.nlp_feature_embedding package](#sk_nlp.nlp_feature_embedding package)

      * [sk_nlp.nlp_feature_embedding.bert module](#sk_nlp.nlp_feature_embedding.bert module)

      * [sk_nlp.nlp_feature_embedding.similarity module](#sk_nlp.nlp_feature_embedding.similarity module)

      * [sk_nlp.nlp_feature_embedding.w2v module](#sk_nlp.nlp_feature_embedding.w2v module)



#sk_nlp.nlp_feature_extract package
**********************************



##sk_nlp.nlp_feature_extract.feature module

0 ä½¿ç”¨acè‡ªåŠ¨æœºç»Ÿè®¡ç»™å®šçš„è¯è¯­çš„è¯é¢‘ 1 è·å–tf-idfç‰¹å¾

class sk_nlp.nlp_feature_extract.feature.CountByAC(pattern_list=[])

   Bases: "object"

   åŸºäºacè‡ªåŠ¨æœºæ¥ç»Ÿè®¡æ¨¡å¼ä¸²

   Parameters:
      **pattern_list** -- åŒ¹é…çš„æ¨¡å¼ä¸²åˆ—è¡¨

   build_tree(pattern_list)

      æ„å»ºæ¨¡å¼ä¸²å‰ç¼€æ ‘

      Parameters:
         **pattern_list** -- æ¨¡å¼ä¸²åˆ—è¡¨

   count(sentence)

      ç»Ÿè®¡sentenceä¸­å…³äºç»™å®šçš„æ¨¡å¼ä¸²çš„é¢‘ç‡

      Parameters:
         **sentence** -- å¥å­

      Returns:
         word_count æ¯ä¸ªå…³é”®è¯å¯¹åº”çš„é¢‘ç‡

      >>> ac = CountByAC(['æ°ä¼¦çš„ä¸ƒ', 'å‘¨æ°ä¼¦çš„', 'ä¸ƒé‡Œé¦™'])
      >>> result = ac.count('å‘¨æ°ä¼¦çš„ä¸ƒé‡Œé¦™ä¸ƒé‡Œé¦™')
      >>> print(result)
      {'å‘¨æ°ä¼¦çš„': 1, 'æ°ä¼¦çš„ä¸ƒ': 1, 'ä¸ƒé‡Œé¦™': 2}

class sk_nlp.nlp_feature_extract.feature.KeyWordExtract

   Bases: "object"

   å…³é”®è¯æŠ½å–ç®—æ³•ï¼ŒåŸºäºtf-idf

   get_tf_idf(sentence_list, model_file)

      åŠ è½½tf-idfæ¨¡å‹ï¼Œè¿”å›sentence_listå¯¹åº”çš„ç‰¹å¾å’Œæ¨¡å‹

      Parameters:
         * **sentence_list** -- å¥å­åˆ—è¡¨ï¼ˆåˆ†è¯åï¼‰

         * **model_file** -- tf-idfæ¨¡å‹æ–‡ä»¶

      Returns:
         tf_idf_model(æ¨¡å‹å®ä¾‹), tfidf_feature(sentence_listå¯¹åº”çš„tf-
         idfç‰¹å¾)

      >>> tf_idf_model, tfidf_feature = kwe.get_tf_idf(['æ°ä¼¦ æ˜¯ å°æ¹¾ æ­Œæ‰‹', 'ä¸ƒé‡Œé¦™ æ˜¯ æ°ä¼¦ åˆ›ä½œ'], file_conf.tf_idf_file_path)
      >>> print(tfidf_feature)
        (0, 4)        0.6316672017376245
        (0, 3)        0.4494364165239821
        (0, 2)        0.6316672017376245
        (1, 3)        0.4494364165239821
        (1, 1)        0.6316672017376245
        (1, 0)        0.6316672017376245

   get_topk_keywords(data_list, topk=200)

      å¾—åˆ°topkä¸ªå…³é”®è¯

      Parameters:
         * **data_list** -- å¥å­åˆ—è¡¨ï¼ˆåˆ†è¯åï¼‰

         * **topk** -- tf-idfé‡è¦åº¦æ’åºåå‰topk

      Returns:
         keywords

      >>> keywords = kwe.get_topk_keywords(['æ°ä¼¦ æ˜¯ å°æ¹¾ æ­Œæ‰‹', 'ä¸ƒé‡Œé¦™ æ˜¯ æ°ä¼¦ åˆ›ä½œ'], topk=1)
      >>> print(keywords)
      [['æ­Œæ‰‹']['åˆ›ä½œ']]

   train_tf_idf(sentence_list, model_file, ngram_range=(1, 1))

      è®­ç»ƒtf-idfæ¨¡å‹ï¼Œä¿å­˜æ¨¡å‹ï¼Œè¿”å›æ¨¡å‹å’Œç‰¹å¾

      Parameters:
         * **sentence_list** -- å¥å­åˆ—è¡¨ï¼ˆåˆ†è¯åï¼‰

         * **model_file** -- tf-idfæ¨¡å‹ä¿å­˜æ–‡ä»¶

      Returns:
         tf_idf_model, tfidf_feature


##sk_nlp.nlp_feature_extract.text_filter module


æ•æ„Ÿè¯æ±‡è¿‡æ»¤æ¨¡å—ï¼Œå…±å®ç°äº†3ä¸ªç±»ï¼šNaiveFilterï¼ŒBSFilterï¼ŒDFAFilter

class sk_nlp.nlp_feature_extract.text_filter.BSFilter

   Bases: "object"

   å®½åº¦ä¼˜å…ˆéå†çš„æ–¹å¼è¿‡æ»¤

   add(keyword)

      æ–°å¢ä¸€ä¸ªæ•æ„Ÿè¯

      :param keyword:æ•æ„Ÿè¯ :return:æ— 

   filter(message, repl='*')

      è¿‡æ»¤æ‰æ•æ„Ÿè¯

      Parameters:
         * **message** -- åŸå§‹çš„è¾“å…¥å¥å­

         * **repl** -- æ•æ„Ÿè¯æ±‡è¢«æ›¿æ¢æˆçš„å­—ç¬¦

      Returns:
         message å±è”½æ‰æ•æ„Ÿè¯æ±‡çš„å¥å­

      >>> f = BSFilter()
      >>> question = "å°æ¹¾æ˜¯ä¸­å›½çš„å—"
      >>> filter_question = f.filter(question)
      >>> print(question, filter_question)
      å°æ¹¾æ˜¯ä¸­å›½çš„å— *æ˜¯ä¸­å›½çš„å—

   parse(path)

      åŠ è½½æ•æ„Ÿè¯æ±‡è¡¨

      Parameters:
         **path** -- è·¯å¾„ä¸º/sk-nlp/data/dirty_word.txt

      Returns:
class sk_nlp.nlp_feature_extract.text_filter.DFAFilter

   Bases: "object"

   DFAå³Deterministic Finite Automatonï¼Œä¹Ÿå°±æ˜¯ç¡®å®šæœ‰ç©·è‡ªåŠ¨æœºã€‚ ç®—æ³•æ ¸
   å¿ƒæ˜¯å»ºç«‹äº†ä»¥æ•æ„Ÿè¯ä¸ºåŸºç¡€çš„è®¸å¤šæ•æ„Ÿè¯æ ‘

   add(keyword)

      æ–°å¢ä¸€ä¸ªæ•æ„Ÿè¯

      :param keyword:æ•æ„Ÿè¯ :return:æ— 

   detect(message)

      åˆ¤æ–­messageæ˜¯å¦åŒ…å«æ•æ„Ÿè¯æ±‡

      :param message:ç”¨æˆ·è¾“å…¥çš„å¥å­ :return: True/False

   filter(message, repl='*')

      è¿‡æ»¤æ‰æ•æ„Ÿè¯

      Parameters:
         * **message** -- åŸå§‹çš„è¾“å…¥å¥å­

         * **repl** -- æ•æ„Ÿè¯æ±‡è¢«æ›¿æ¢æˆçš„å­—ç¬¦

      Returns:
         message å±è”½æ‰æ•æ„Ÿè¯æ±‡çš„å¥å­

      >>> f = DFAFilter()
      >>> question = "å°æ¹¾æ˜¯ä¸­å›½çš„å—"
      >>> filter_question = f.filter(question)
      >>> print(question, filter_question)
      å°æ¹¾æ˜¯ä¸­å›½çš„å— *æ˜¯ä¸­å›½çš„å—

   parse(path)

      åŠ è½½æ•æ„Ÿè¯æ±‡è¡¨

      Parameters:
         **path** -- è·¯å¾„ä¸º/sk-nlp/data/dirty_word.txt

      Returns:
class sk_nlp.nlp_feature_extract.text_filter.NaiveFilter

   Bases: "object"

   æ™®é€šçš„è¿‡æ»¤æ–¹å¼ï¼šä½¿ç”¨é›†åˆçš„æ–¹å¼è¿‡æ»¤ï¼Œæ—¶é—´å¤æ‚åº¦è·Ÿé›†åˆçš„å¤§å°æœ‰å…³

   filter(message, repl='*')

      è¿‡æ»¤æ‰æ•æ„Ÿè¯

      Parameters:
         * **message** -- åŸå§‹çš„è¾“å…¥å¥å­

         * **repl** -- æ•æ„Ÿè¯æ±‡è¢«æ›¿æ¢æˆçš„å­—ç¬¦

      Returns:
         messageï¼šå±è”½æ‰æ•æ„Ÿè¯æ±‡çš„å¥å­

      >>> f = NaiveFilter()
      >>> question = "å°æ¹¾æ˜¯ä¸­å›½çš„å—"
      >>> filter_question = f.filter(question)
      >>> print(question, filter_question)
      å°æ¹¾æ˜¯ä¸­å›½çš„å— *æ˜¯ä¸­å›½çš„å—

   parse(path)

      åŠ è½½æ•æ„Ÿè¯æ±‡è¡¨

      Parameters:
         **path** -- è·¯å¾„ä¸º/sk-nlp/data/dirty_word.txt

      Returns:

sk_nlp.nlp_feature_extract.tokenizer module
===========================================

è¯è¯­ç²’åº¦çš„æ“ä½œæ¨¡å—ï¼šåˆ†è¯ï¼Œå»åœç”¨è¯ï¼ŒåŒä¹‰è¯æ—è½¬æ¢

class sk_nlp.nlp_feature_extract.tokenizer.SentenceCut(is_lower=True, stopword_list=[], use_chinese_synonyms=False)

   Bases: "object"

   å¥å­åˆ†è¯æ“ä½œç±» ç›®å‰é›†æˆäº†jiebaåˆ†è¯

   cut_word(sentence_list)

      å¯¹ä¼ è¿›æ¥çš„å¥å­è¿›è¡Œåˆ†è¯

      :param sentence_list:['æˆ‘çˆ±ä¸­å›½', 'æˆ‘æ˜¯ä¸­å›½äºº']
      :return:seg_lists [['æˆ‘', 'çˆ±', 'ä¸­å›½'], ['æˆ‘', 'æ˜¯', 'ä¸­å›½', '
      äºº']]  token_count {'æˆ‘': 2, 'çˆ±': 1, 'ä¸­å›½': 2, 'æ˜¯': 1, 'äºº':
      1}

      >>> sen_cut = SentenceCut(use_chinese_synonyms=True)
      >>> seg_lists, token_count = sen_cut.cut_word(['æˆ‘çˆ±baidu', 'æˆ‘æ˜¯ä¸­å›½äºº'])
      >>> print(seg_lists, token_count)
      [['æˆ‘', 'çˆ±', 'ç™¾åº¦'], ['æˆ‘', 'æ˜¯', 'ä¸­å›½', 'äºº']]
      {'æˆ‘': 2, 'çˆ±': 1, 'ç™¾åº¦': 1, 'æ˜¯': 1, 'ä¸­å›½': 1, 'äºº': 1}

   load_chinese_synonyms()

      åŠ è½½åŒä¹‰è¯æ—

      Returns:
         union_find ï¼ˆå¹¶æŸ¥é›†å®ä¾‹ï¼‰ï¼Œword_listï¼ˆåŒä¹‰è¯æ—æ‰€æœ‰çš„å•è¯é›†åˆ
         ï¼‰

class sk_nlp.nlp_feature_extract.tokenizer.StopWord(source='', define_stop_word=[])

   Bases: "object"

   åœç”¨è¯æ“ä½œç±»ï¼š åœç”¨è¯æ±‡è¡¨è·¯å¾„å­˜æ”¾åœ¨ sk-nlp/data/stopword

   load_stop_word()

      æ ¹æ®ä¸åŒçš„self.sourceåŠ è½½ä¸åŒçš„åœç”¨è¯è¡¨

      Returns:
         stop_word_list åœç”¨è¯åˆ—è¡¨

   merge_stop_word(define_stop_word)

      å°†ç”¨æˆ·è‡ªå®šä¹‰çš„åœç”¨è¯å’Œç”¨æˆ·æŒ‡å®šçš„é€šç”¨è¯åº“åˆå¹¶æˆä¸€ä¸ªlist

      Parameters:
         **define_stop_word** -- ç”¨æˆ·ç»™çš„è‡ªå®šä¹‰åœç”¨è¯åˆ—è¡¨ list

      Returns:
         stop_word_list åœç”¨è¯åˆ—è¡¨

#sk_nlp.nlp_feature_embedding package
************************************



sk_nlp.nlp_feature_embedding.bert module
========================================

bertåŸºæœ¬æ¨¡å‹åŠ è½½

class sk_nlp.nlp_feature_embedding.bert.MaskLayer(output_dim=768, **kwargs)

   Bases: "keras.engine.base_layer.Layer"

   mask å±‚ï¼Œå±è”½æ‰seg_idä¸º0çš„è¯è¯­

   build(input_shape)

      åˆ›å»ºå±‚çš„æƒé‡

      :param input_shape:Keras tensor (future input to layer) or
      list/tuple of Keras tensors :return:

   call(x)

      This is where the layer's logic lives.

      # Arguments
         inputs: Input tensor, or list/tuple of input tensors.
         >>**<<kwargs: Additional keyword arguments.

      # Returns
         A tensor or list/tuple of tensors.

   compute_output_shape(input_shape)

      Computes the output shape of the layer.

      Assumes that the layer will be built to match that input shape
      provided.

      # Arguments
         input_shape: Shape tuple (tuple of integers)
            or list of shape tuples (one per output tensor of the
            layer). Shape tuples can include None for free dimensions,
            instead of an integer.

      # Returns
         An output shape tuple.

class sk_nlp.nlp_feature_embedding.bert.ReverseMaskLayer(**kwargs)

   Bases: "keras.engine.base_layer.Layer"

   åè½¬ mask å±‚ï¼Œå±è”½æ‰seg_idä¸º1çš„è¯è¯­

   call(x)

      This is where the layer's logic lives.

      # Arguments
         inputs: Input tensor, or list/tuple of input tensors.
         >>**<<kwargs: Additional keyword arguments.

      # Returns
         A tensor or list/tuple of tensors.

   compute_output_shape(input_shape)

      Computes the output shape of the layer.

      Assumes that the layer will be built to match that input shape
      provided.

      # Arguments
         input_shape: Shape tuple (tuple of integers)
            or list of shape tuples (one per output tensor of the
            layer). Shape tuples can include None for free dimensions,
            instead of an integer.

      # Returns
         An output shape tuple.

class sk_nlp.nlp_feature_embedding.bert.SepLayer(**kwargs)

   Bases: "keras.engine.base_layer.Layer"

   sep mask å±‚ï¼Œå±è”½æ‰sepä½ç½®çš„è¾“å‡º

   call(x)

      This is where the layer's logic lives.

      # Arguments
         inputs: Input tensor, or list/tuple of input tensors.
         >>**<<kwargs: Additional keyword arguments.

      # Returns
         A tensor or list/tuple of tensors.

   compute_output_shape(input_shape)

      Computes the output shape of the layer.

      Assumes that the layer will be built to match that input shape
      provided.

      # Arguments
         input_shape: Shape tuple (tuple of integers)
            or list of shape tuples (one per output tensor of the
            layer). Shape tuples can include None for free dimensions,
            instead of an integer.

      # Returns
         An output shape tuple.

sk_nlp.nlp_feature_embedding.bert.build_model_feature(origin_model, use_cls=False)

   æ­å»ºæ–°çš„å¥å­æ¨¡å‹

   Parameters:
      * **origin_model** -- åŸå§‹æ¨¡å‹ï¼Œä¸€èˆ¬ä¸ºbert

      * **use_cls** -- æ˜¯å¦ä½¿ç”¨clsä½ç½®çš„è¾“å‡º

   Returns:
      modelï¼šæ–°æ¨¡å‹

sk_nlp.nlp_feature_embedding.bert.encoder(model, data_list, dict_path='/machinelearn/wzh/sk_nlp/sk_nlp/model/bert/chinese_L-12_H-768_A-12/vocab.txt')

   ä½¿ç”¨å¥å‘é‡æ¨¡å‹ï¼Œå°†å¥å­è½¬ç æˆå¥å‘é‡

   Parameters:
      * **model** -- æ¨¡å‹

      * **data_list** -- å¥å­åˆ—è¡¨ï¼ˆæ²¡æœ‰åˆ†è¯ï¼‰

      * **dict_path** -- bertæ¨¡å‹è¯æ±‡è¡¨

   Returns:
      data_listä¸­çš„æ¯ä¸ªå¥å­å¯¹åº”çš„å¥å‘é‡åˆ—è¡¨

   >>> origin_model = load_bert_model()
   >>> new_model = build_model_feature(origin_model)
   >>> question_list = ["æˆ‘çˆ±è¿™ä¸ªä¼Ÿå¤§çš„ä¸–ç•Œ", "æ¬£èµä¸–ç•Œçš„é£æ™¯"]
   >>> sen_vector_lists = encoder(new_model, question_list)
   >>> print(sen_vector_lists.shape)

sk_nlp.nlp_feature_embedding.bert.load_bert_model(with_mlm=True, with_pool=False, return_keras_model=True, config_path='/machinelearn/wzh/sk_nlp/sk_nlp/model/bert/chinese_L-12_H-768_A-12/bert_config.json', checkpoint_path='/machinelearn/wzh/sk_nlp/sk_nlp/model/bert/chinese_L-12_H-768_A-12/bert_model.ckpt')

   åŠ è½½bert æ¨¡å‹

   Parameters:
      * **with_mlm** -- æ˜¯å¦æ­£åˆ™åŒ–

      * **with_pool** -- æ˜¯å¦æ± åŒ–

      * **return_keras_model** -- è¿”å›çš„æ˜¯keras model è¿˜æ˜¯ tensorflow
        æ¨¡å‹

      * **config_path** -- bert æ¨¡å‹é…ç½®æ–‡ä»¶è·¯å¾„

      * **checkpoint_path** -- bert æ¨¡å‹è·¯å¾„

   Returns:
sk_nlp.nlp_feature_embedding.bert.masked_crossentropy(y_true, y_pred)

   maskæ‰éé¢„æµ‹éƒ¨åˆ†ï¼Œè®¡ç®—äº¤å‰ç†µ

   Parameters:
      * **y_true** -- çœŸå®çš„Yæ ‡ç­¾

      * **y_pred** -- é¢„æµ‹çš„Yæ ‡ç­¾

   Returns:
      æŸå¤±å€¼


sk_nlp.nlp_feature_embedding.similarity module
==============================================

è®¡ç®—å„ç§è·ç¦»

sk_nlp.nlp_feature_embedding.similarity.get_distance_sim_matrix(matrix1, matrix2, metric='cosine')

   è¿”å›2ä¸ªçŸ©é˜µçš„å„ç§è·ç¦»å’Œç›¸ä¼¼åº¦

   Parameters:
      * **matrix1** -- å¥å­å‘é‡1

      * **matrix2** -- å¥å­å‘é‡2

      * **metric** -- 'braycurtis', 'canberra', 'chebyshev',
        'cityblock', 'correlation',

   'cosine', 'dice', 'euclidean', 'hamming', 'jaccard',
   'jensenshannon', 'kulsinski', 'mahalanobis', 'matching',
   'minkowski', 'rogerstanimoto', 'russellrao', 'seuclidean',
   'sokalmichener', 'sokalsneath', 'sqeuclidean', 'wminkowski', 'yule'
   :return:

sk_nlp.nlp_feature_embedding.similarity.get_edit_distance(query_sen_list, candidate_sen_list)

   è®¡ç®—ç¼–è¾‘è·ç¦»

   Parameters:
      * **query_sen_list** -- å¦‚['æˆ‘çˆ±ä¸­å›½', 'ç¾å›½æ€»ç»Ÿç‰¹æœ—æ™®']

      * **candidate_sen_list** -- å¦‚['æˆ‘çˆ±åœ°çƒ', 'ç¾å›½æ€»ç»Ÿæ‹œç™»']

   Returns:
sk_nlp.nlp_feature_embedding.similarity.get_edit_similarity(distance_matrix, norm=True)

   å…ˆåè½¬ç¼–è¾‘è·ç¦»çŸ©é˜µï¼Œå¾—åˆ°ç¼–è¾‘ç›¸ä¼¼åº¦çŸ©é˜µï¼Œç„¶åå¯ä»¥é€‰æ‹©å½’ä¸€åŒ–

   Parameters:
      * **distance_matrix** -- è·ç¦»çŸ©é˜µ

      * **norm** -- True/False

   Returns:
sk_nlp.nlp_feature_embedding.similarity.get_jaccard_sim(sen_list1, sen_list2, norm=False)

   è·å¾—æ°å¡å¾·ç›¸ä¼¼åº¦

   Parameters:
      * **sen_list1** -- [['æˆ‘', 'çˆ±','ä¸­å›½'], ['ç¾å›½', 'æ€»ç»Ÿ', 'ç‰¹æœ—
        æ™®']]

      * **sen_list2** -- [['æˆ‘', 'çˆ±','åœ°çƒ'], ['ç¾å›½', 'æ€»ç»Ÿ', 'æ‹œç™»
        ']]

   :param norm:æ˜¯å¦å¯¹ç»“æœè¿›è¡Œå½’ä¸€åŒ– :return:

sk_nlp.nlp_feature_embedding.similarity.match_topk(sim_matrix, topk=1, order=0)

   è¿”å›ç›¸ä¼¼åº¦çŸ©é˜µå‰topk/æˆ–è€…åtopk

   Parameters:
      * **sim_matrix** --

      * **topk** --

      * **order** --

   Returns:
sk_nlp.nlp_feature_embedding.similarity.normalization(matrix, reversed=True)

   å½’ä¸€åŒ–çŸ©é˜µï¼ŒæŒ‰ç…§æœ€åä¸€ä¸ªç»´åº¦

   Parameters:
      * **matrix** --

      * **reversed** --

   Returns:

sk_nlp.nlp_feature_embedding.w2v module
=======================================

ä¼ ç»Ÿçš„w2væ¨¡å‹:åŒ…å«skip-gramå’Œcbow ç›®å‰æœ‰ä¸€ä¸ªä»wikiè¯­æ–™è®­ç»ƒå‡ºæ¥çš„100ç»´
åº¦çš„skip-gramæ¨¡å‹

class sk_nlp.nlp_feature_embedding.w2v.WordEmbedding(model_file_path='/machinelearn/wzh/sk_nlp/sk_nlp/model/w2v/skip_gram_wiki2Vec.h5', embedding_dim=100)

   Bases: "object"

   fine_tune(new_seg_list, model_file_path)

      åŸºäºå·²æœ‰çš„w2væ¨¡å‹ï¼Œä½¿ç”¨å…¶ä»–è¯­æ–™è¿›è¡Œå¾®è°ƒã€‚ç„¶åä¿å­˜æ¨¡å‹è·¯å¾„ã€‚

      Parameters:
         * **new_seg_list** -- æ–°å¥å­ï¼ˆåˆ†è¯åï¼‰

         * **model_file_path** -- æ¨¡å‹çš„ä¿å­˜è·¯å¾„

      Returns:
      >>> model = WordEmbedding()
      >>> model.get_embedding()
      >>> new_seg_list = [['æˆ‘', 'çˆ±','ä¸­å›½'], ['ç¾å›½', 'æ€»ç»Ÿ', 'ç‰¹æœ—æ™®']]
      >>> model.fine_tune(new_seg_list, file_conf.ft_wiki_sg_file_path)

   get_embedding()

      è·å–è¯å‘é‡æ¨¡å‹çš„ä¿¡æ¯

      Returns:
         embedding_matrix:è¯å‘é‡çŸ©é˜µï¼›index_wordï¼šç´¢å¼•åˆ°å•è¯çš„æ˜ å°„ï¼›
         word_indexï¼šå•è¯åˆ°ç´¢å¼•çš„æ˜ å°„

   op2model()

      ç”±äºw2vçš„æ¥å£å¤ªå¤šï¼Œä¸å¤ªå¥½å°è£… è¿™é‡Œç»™å‡ºäº†æ¨¡å‹çš„ä¸€äº›å¸¸ç”¨æ“ä½œèŒƒä¾‹

      Returns:
   train_vec(sentence_list, model_file_path, window=5, min_count=5, sg=0)

      ä½¿ç”¨w2vè®­ç»ƒè¯å‘é‡

      Parameters:
         * **sentence_list** -- å¥å­åˆ—è¡¨ï¼Œ[['æˆ‘', 'çˆ±','ä¸­å›½'], ['ç¾å›½
           ', 'æ€»ç»Ÿ', 'ç‰¹æœ—æ™®']]

         * **model_file_path** -- æ¨¡å‹ä¿å­˜è·¯å¾„

         * **window** -- æ»‘åŠ¨çª—å£

         * **min_count** -- æœ€å°è¯é¢‘

         * **sg** -- 0æ˜¯ä½¿ç”¨cbow, 1æ˜¯ä½¿ç”¨è·³å­—æ¨¡å‹

      Returns:

Module contents
===============


Module contents
===============


More Resources
--------------

-   [where is bert pre-train model]  https://github.com/google-research/bert
-   [where is stopwords corpus]  https://github.com/goto456/stopwords
-   [Official Python Packaging User Guide](https://packaging.python.org)
-   [The Hitchhiker's Guide to Packaging]

License
-------

This is free and unencumbered software released into the public domain.
Anyone is free to copy, modify, publish, use, compile, sell, or
distribute this software, either in source code form or as a compiled
binary, for any purpose, commercial or non-commercial, and by any means.



